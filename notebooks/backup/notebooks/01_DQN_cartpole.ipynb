{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import gymnasium as gym\n",
                "import torch\n",
                "import numpy as np\n",
                "import wandb\n",
                "from collections import deque\n",
                "from src.utils.util import ShellColor as sc\n",
                "\n",
                "print(f\"{sc.COLOR_PURPLE}Gym version:{sc.ENDC} {gym.__version__}\")\n",
                "print(f\"{sc.COLOR_PURPLE}Pytorch version:{sc.ENDC} {torch.__version__}\")\n",
                "\n",
                "from src.agents.dqn_agent import DQNAgent\n",
                "from src.utils import util as rl_util"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "env_name = \"CartPole-v1\"\n",
                "env = gym.make(env_name)\n",
                "rl_util.print_env_info(env=env)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "config = rl_util.create_config()\n",
                "config[\"batch_size\"] = 32\n",
                "config[\"buffer_size\"] = 50000\n",
                "config[\"gamma\"] = 0.99\n",
                "config[\"target_update_frequency\"] = 1000\n",
                "config[\"learning_starts\"]=1000\n",
                "config[\"learning_frequency\"] = 1\n",
                "config[\"lr\"] = 1e-4\n",
                "config[\"start_training_step\"] = 1000\n",
                "config[\"mean_reward_bound\"] = 490\n",
                "config[\"print_frequency\"] = 100"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "wandb.init(project=\"DQN-cartpole\", config=config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "agent = DQNAgent(\n",
                "    obs_space_shape=env.observation_space.shape,\n",
                "    action_space_dims=env.action_space.n,\n",
                "    is_atari=False,\n",
                "    config=config,\n",
                ")\n",
                "\n",
                "print(agent.config)\n",
                "print(type(agent.memory))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "save_dir = \"result/DQN/cartpole/\"\n",
                "rl_util.create_directory(save_dir)\n",
                "save_model_name = save_dir + env_name + \"_mean_score.pt\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "total_rewards = []\n",
                "losses = []\n",
                "frame_idx = 0\n",
                "for i_episode in range(agent.config.num_steps):\n",
                "    obs, info = env.reset()\n",
                "    avg_loss = 0\n",
                "    len_game_progress = 0\n",
                "    cur_reward = 0\n",
                "    mean_rewards = deque([], maxlen=5)\n",
                "    while True:\n",
                "        # env.render()\n",
                "        frame_idx += 1\n",
                "        eps = agent.decay_epsilon(frame_idx)\n",
                "        action = agent.select_action(obs, eps)\n",
                "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
                "        done = terminated or truncated\n",
                "        agent.store_transition(obs, action, reward, next_obs, done)\n",
                "\n",
                "        if len(agent.memory.replay_buffer) > 1000:\n",
                "            loss = agent.update()\n",
                "            avg_loss += loss\n",
                "\n",
                "        obs = next_obs\n",
                "        len_game_progress += 1\n",
                "        cur_reward += reward\n",
                "        if done:\n",
                "            break\n",
                "        \n",
                "    agent.update_target_network()\n",
                "\n",
                "    avg_loss /= len_game_progress\n",
                "    losses.append(avg_loss)\n",
                "\n",
                "    mean_rewards.append(cur_reward)\n",
                "    total_rewards.append(cur_reward)\n",
                "    mean_rewards = np.mean(mean_rewards)\n",
                "\n",
                "    if (i_episode) % 20 == 0:\n",
                "        print(\n",
                "            f\"episode: {i_episode} | mean_rewards: {mean_rewards:.4f} | loss: {avg_loss:.4f} | epsilon: {eps:.4f}\"\n",
                "        )\n",
                "        wandb.log({\n",
                "            \"episode\": i_episode,\n",
                "            \"mean_rewards\": mean_rewards,\n",
                "            \"cur_rewards\" : cur_reward,\n",
                "            \"loss\": avg_loss,\n",
                "            \"epsilon\": eps\n",
                "        })\n",
                "        \n",
                "    if mean_rewards > 490:\n",
                "        current_time = rl_util.get_current_time_string()\n",
                "        save_model_name = save_dir + \"checkpoint_\" + current_time + \".pt\"\n",
                "        print(f\"Save model {save_model_name} | episode is {(i_episode)}\")\n",
                "        torch.save(agent.policy_network.state_dict(), save_model_name)\n",
                "        break\n",
                "\n",
                "env.close()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = rl_util.init_2d_figure(\"Reward\")\n",
                "rl_util.plot_graph(\n",
                "    ax,\n",
                "    total_rewards,\n",
                "    title=\"reward\",\n",
                "    ylabel=\"reward\",\n",
                "    save_dir_name=save_dir,\n",
                "    is_save=True,\n",
                ")\n",
                "rl_util.show_figure()\n",
                "fig, ax = rl_util.init_2d_figure(\"Loss\")\n",
                "rl_util.plot_graph(\n",
                "    ax, losses, title=\"loss\", ylabel=\"loss\", save_dir_name=save_dir, is_save=True\n",
                ")\n",
                "rl_util.show_figure()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "env = gym.make(env_name, render_mode=\"human\")\n",
                "\n",
                "test_agent = DQNAgent(\n",
                "    obs_space_shape=env.observation_space.shape,\n",
                "    action_space_dims=env.action_space.n,\n",
                "    is_atari=False,\n",
                "    config=config\n",
                ")\n",
                "\n",
                "file_name = save_dir + \"CartPole-v1_mean_score.pt\"\n",
                "test_agent.policy_network.load_state_dict(torch.load(file_name))\n",
                "\n",
                "for i_episode in range(1):\n",
                "    state, _ = env.reset()\n",
                "    test_reward = 0\n",
                "    while True:\n",
                "        env.render()\n",
                "        action = test_agent.select_action(state, 0.)\n",
                "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
                "        test_reward += reward\n",
                "        state = next_state\n",
                "        done = terminated or truncated\n",
                "        if done:\n",
                "            break\n",
                "    print(f\"{i_episode} episode Total Reward: {test_reward}\")\n",
                "env.close()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "test",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.15"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "2079ad370c17dffbaa1cef888f8327d4562759f468d0bda43c9a0b46b2857d81"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
